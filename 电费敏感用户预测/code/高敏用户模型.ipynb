{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "import jieba\n",
    "import pickle\n",
    "from numpy import log\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/12.png\" style=\"width:800px;height:360px;float:left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/13.png\" style=\"width:800px;height:360px;float:left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/14.png\" style=\"width:800px;height:360px;float:left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../rawdata/'\n",
    "#95598 train \n",
    "file_jobinfo_train = '01_arc_s_95598_wkst_train.zip' \n",
    "# 95598 test\n",
    "file_jobinfo_test = '01_arc_s_95598_wkst_test.zip'    \n",
    "# 通话信息记录\n",
    "file_comm = '02_s_comm_rec.tsv'\n",
    "# 应收电费信息表 train\n",
    "file_flow_train = '09_arc_a_rcvbl_flow.tsv'\n",
    "# 应收电费信息表 test\n",
    "file_flow_test = '09_arc_a_rcvbl_flow_test.tsv'\n",
    "# 训练集正例\n",
    "file_label = 'train_label.csv'\n",
    "# 测试集\n",
    "file_test = 'test_to_predict.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_info = pd.read_csv(data_path + 'processed_' + file_jobinfo_train, sep='\\t', encoding='utf-8', quoting=csv.QUOTE_NONE)\n",
    "# 过滤CUST_NO为空的用户\n",
    "train_info = train_info.loc[~train_info.CUST_NO.isnull()]\n",
    "train_info['CUST_NO'] = train_info.CUST_NO.astype(np.int64)\n",
    "# 构建用户索引\n",
    "train = train_info.CUST_NO.value_counts().to_frame().reset_index()\n",
    "train.columns = ['CUST_NO', 'counts_of_jobinfo']\n",
    "temp = pd.read_csv(data_path + file_label, header=None)\n",
    "temp.columns = ['CUST_NO']\n",
    "train['label'] = 0\n",
    "train.loc[train.CUST_NO.isin(temp.CUST_NO), 'label'] = 1\n",
    "train = train[['CUST_NO', 'label', 'counts_of_jobinfo']]\n",
    "\n",
    "test_info = pd.read_csv(data_path + 'processed_' + file_jobinfo_test, sep='\\t', encoding='utf-8', quoting=csv.QUOTE_NONE)\n",
    "test = test_info.CUST_NO.value_counts().to_frame().reset_index()\n",
    "test.columns = ['CUST_NO', 'counts_of_jobinfo']\n",
    "test['label'] = -1\n",
    "test = test[['CUST_NO', 'label', 'counts_of_jobinfo']]\n",
    "\n",
    "df = train.append(test).copy()\n",
    "labels = df.copy()\n",
    "del temp, train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "得到高敏数据集，去掉离群点情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始数据中的高敏感度用户分布情况如下：\n",
      "训练集： 253407\n",
      "正样本: 64139\n",
      "负样本: 189268\n",
      "-----------------------\n",
      "测试集： 43434\n"
     ]
    }
   ],
   "source": [
    "df = df.loc[df.counts_of_jobinfo != 1].copy()\n",
    "df = df.loc[df.counts_of_jobinfo <= 10].copy()\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "train = df.loc[df.label != -1]\n",
    "test = df.loc[df.label == -1]\n",
    "print('原始数据中的高敏感度用户分布情况如下：')\n",
    "print('训练集：',train.shape[0])\n",
    "print('正样本:',train.loc[train.label == 1].shape[0])\n",
    "print('负样本:',train.loc[train.label == 0].shape[0])\n",
    "print('-----------------------')\n",
    "print('测试集：',test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构造统计数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "构造表1 date和topic的敏感度特征...\n"
     ]
    }
   ],
   "source": [
    "jobinfo = train_info.append(test_info).copy()\n",
    "jobinfo = jobinfo.merge(labels[['CUST_NO', 'label']], on='CUST_NO', how='left')\n",
    "jobinfo['date'] = jobinfo.HANDLE_TIME.apply(lambda x:(str(x).split()[0]))\n",
    "print('构造表1 date和topic的敏感度特征...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "日期特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "train = jobinfo[jobinfo.label != -1]\n",
    "ratio = {}\n",
    "a = 0.001\n",
    "for i in train.date.unique():\n",
    "    ratio[i] = (len(train.loc[(train.date == i) & (train.label == 1)]) + a) / (len(train.loc[train.date == i]) + 2*a)\n",
    "jobinfo['date_ratio'] = jobinfo.date.map(ratio)\n",
    "\n",
    "df['sum_date_ratio'] = df.CUST_NO.map(jobinfo.groupby('CUST_NO').date_ratio.sum())\n",
    "df['mean_date_ratio'] = df.CUST_NO.map(jobinfo.groupby('CUST_NO').date_ratio.mean())\n",
    "df['min_date_ratio'] = df.CUST_NO.map(jobinfo.groupby('CUST_NO').date_ratio.min())\n",
    "df['max_date_ratio'] = df.CUST_NO.map(jobinfo.groupby('CUST_NO').date_ratio.max())\n",
    "df['std_date_ratio'] = df.CUST_NO.map(jobinfo.groupby('CUST_NO').date_ratio.std())\n",
    "df['median_date_ratio'] = df.CUST_NO.map(jobinfo.groupby('CUST_NO').date_ratio.median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "topic特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "pattern = re.compile('【[^【】]*】')\n",
    "def get_topic(x):\n",
    "    finds = pattern.findall(x)\n",
    "    if len(finds) == 0:\n",
    "        return '-1'\n",
    "    else:\n",
    "        return finds[0]\n",
    "jobinfo['topic'] = jobinfo.ACCEPT_CONTENT.apply(lambda x: get_topic(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同样计算比例特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "train = jobinfo[jobinfo.label != -1]\n",
    "ratio = {}\n",
    "a = 0.001\n",
    "for i in train.topic.unique():\n",
    "    ratio[i] = (len(train.loc[(train.topic == i) & (train.label == 1)]) + a) / (len(train.loc[train.topic == i]) + 2*a)\n",
    "    \n",
    "topics = jobinfo.topic.value_counts().to_frame().reset_index()\n",
    "topics.columns = ['topic', 'counts']\n",
    "topics['topic_ratio'] = topics.topic.map(ratio)\n",
    "topics = topics.loc[(topics.counts > 4) & (~topics.topic_ratio.isnull())]\n",
    "jobinfo = jobinfo.merge(topics[['topic', 'topic_ratio']], on='topic', how='left')\n",
    "jobinfo.topic_ratio.fillna(topics.topic_ratio.median(), inplace=True)\n",
    "\n",
    "df['sum_topic_ratio'] = df.CUST_NO.map(jobinfo.groupby('CUST_NO').topic_ratio.sum())\n",
    "df['mean_topic_ratio'] = df.CUST_NO.map(jobinfo.groupby('CUST_NO').topic_ratio.mean())\n",
    "df['min_topic_ratio'] = df.CUST_NO.map(jobinfo.groupby('CUST_NO').topic_ratio.min())\n",
    "df['max_topic_ratio'] = df.CUST_NO.map(jobinfo.groupby('CUST_NO').topic_ratio.max())\n",
    "df['std_topic_ratio'] = df.CUST_NO.map(jobinfo.groupby('CUST_NO').topic_ratio.std())\n",
    "df['median_topic_ratio'] = df.CUST_NO.map(jobinfo.groupby('CUST_NO').topic_ratio.median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "日期特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "comm = pd.read_csv(data_path + file_comm, sep='\\t')\n",
    "comm.drop_duplicates(inplace=True)\n",
    "comm = comm.loc[comm.APP_NO.isin(jobinfo.ID)]\n",
    "comm = comm.rename(columns={'APP_NO':'ID'})\n",
    "comm = comm.merge(jobinfo[['ID', 'CUST_NO']], on='ID', how='left')\n",
    "comm['REQ_BEGIN_DATE'] = comm.REQ_BEGIN_DATE.apply(lambda x:pd.to_datetime(x))\n",
    "comm['REQ_FINISH_DATE'] = comm.REQ_FINISH_DATE.apply(lambda x:pd.to_datetime(x))\n",
    "\n",
    "# 过滤\n",
    "comm = comm.loc[~(comm.REQ_BEGIN_DATE > comm.REQ_FINISH_DATE)].copy()\n",
    "df = df.loc[df.CUST_NO.isin(comm.CUST_NO)].copy()\n",
    "\n",
    "comm['holding_time'] = comm['REQ_FINISH_DATE'] - comm['REQ_BEGIN_DATE']\n",
    "comm['holding_time_seconds'] = comm.holding_time.apply(lambda x:x.seconds)\n",
    "\n",
    "df['counts_of_comm'] = df.CUST_NO.map(comm.groupby('CUST_NO').size())\n",
    "df['min_holding_time_seconds'] = df.CUST_NO.map(comm.groupby('CUST_NO').holding_time_seconds.min())\n",
    "df['min_holding_time_seconds'] = df.min_holding_time_seconds.apply(lambda x:log(x+1))\n",
    "df['max_holding_time_seconds'] = df.CUST_NO.map(comm.groupby('CUST_NO').holding_time_seconds.max())\n",
    "df['max_holding_time_seconds'] = df.max_holding_time_seconds.apply(lambda x:log(x+1))\n",
    "df['sum_holding_time_seconds'] = df.CUST_NO.map(comm.groupby('CUST_NO').holding_time_seconds.sum())\n",
    "df['sum_holding_time_seconds'] = df.sum_holding_time_seconds.apply(lambda x:log(x+1))\n",
    "df['std_holding_time_seconds'] = df.CUST_NO.map(comm.groupby('CUST_NO').holding_time_seconds.std())\n",
    "df['std_holding_time_seconds'] = df.std_holding_time_seconds.apply(lambda x:log(x+1))\n",
    "df['median_holding_time_seconds'] = df.CUST_NO.map(comm.groupby('CUST_NO').holding_time_seconds.median())\n",
    "df['median_holding_time_seconds'] = df.median_holding_time_seconds.apply(lambda x:log(x+1))\n",
    "df['mean_holding_time_seconds'] = df['sum_holding_time_seconds'] / df['counts_of_comm']\n",
    "\n",
    "df['comm_not_equal_jobinfo'] = 0\n",
    "df.loc[df.counts_of_jobinfo != df.counts_of_comm, 'comm_not_equal_jobinfo'] = 1\n",
    "\n",
    "df['counts_jobinfo_jianqu_comm'] = df['counts_of_jobinfo'] - df['counts_of_comm']\n",
    "del comm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BUSI_TYPE_CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "jobinfo = jobinfo.loc[jobinfo.CUST_NO.isin(df.CUST_NO)].copy()\n",
    "jobinfo.reset_index(drop=True, inplace=True)\n",
    "################\n",
    "# CUST_NO\n",
    "################\n",
    "# rank\n",
    "df['rank_CUST_NO'] = df.CUST_NO.rank(method='max')\n",
    "df['rank_CUST_NO'] = MinMaxScaler().fit_transform(df.rank_CUST_NO.reshape(-1, 1))\n",
    "################\n",
    "# BUSI_TYPE_CODE\n",
    "################\n",
    "df['nunique_BUSI_TYPE'] = df.CUST_NO.map(jobinfo.groupby('CUST_NO').BUSI_TYPE_CODE.nunique())\n",
    "df['counts_divide_busi'] =  df['counts_of_jobinfo'] / df['nunique_BUSI_TYPE']\n",
    "df.drop(['nunique_BUSI_TYPE'], axis=1, inplace=True)\n",
    "# count\n",
    "temp = jobinfo[['CUST_NO']]\n",
    "temp = pd.concat([temp, pd.get_dummies(jobinfo.BUSI_TYPE_CODE, prefix='count_BUSI_TYPE_CODE')], axis=1)\n",
    "temp = temp.groupby('CUST_NO').sum()\n",
    "temp.reset_index(inplace=True)\n",
    "df = df.merge(temp, on='CUST_NO', how='left')\n",
    "# ratio\n",
    "for i in jobinfo.BUSI_TYPE_CODE.unique(): \n",
    "    df['ratio_BUSI_TYPE_CODE_{}'.format(i)] = df['count_BUSI_TYPE_CODE_{}'.format(i)] / df.counts_of_jobinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "URBAN_RURAL_FLAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "jobinfo['URBAN_RURAL_FLAG'].fillna(-1, inplace=True)\n",
    "df['nunique_URBAN'] = df.CUST_NO.map(jobinfo.groupby('CUST_NO').URBAN_RURAL_FLAG.nunique())\n",
    "df['ratio_urban'] =  df['counts_of_jobinfo'] / df['nunique_URBAN']\n",
    "# count\n",
    "temp = jobinfo[['CUST_NO']]\n",
    "temp = pd.concat([temp, pd.get_dummies(jobinfo.URBAN_RURAL_FLAG, prefix='count_URBAN_RURAL_FLAG')], axis=1)\n",
    "temp = temp.groupby('CUST_NO').sum()\n",
    "temp.reset_index(inplace=True)\n",
    "df = df.merge(temp, on='CUST_NO', how='left')\n",
    "# ratio\n",
    "for i in jobinfo.URBAN_RURAL_FLAG.unique(): \n",
    "    df['ratio_URBAN_RURAL_FLAG_{}'.format(i)] = df['count_URBAN_RURAL_FLAG_{}'.format(i)] / df.counts_of_jobinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ORG_NO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df['nunique_ORG_NO'] = df.CUST_NO.map(jobinfo.groupby('CUST_NO').ORG_NO.nunique())\n",
    "df['nunique_ORG_NO_divide_counts'] = df['nunique_ORG_NO'] / df['counts_of_jobinfo']\n",
    "df['counts_divide_nunique_ORG_NO'] = df['counts_of_jobinfo'] / df['nunique_ORG_NO']\n",
    "# count\n",
    "temp = jobinfo[['CUST_NO']]\n",
    "temp = pd.concat([temp, pd.get_dummies(jobinfo.ORG_NO, prefix='count_ORG_NO')], axis=1)\n",
    "temp = temp.groupby('CUST_NO').sum()\n",
    "temp.reset_index(inplace=True)\n",
    "df = df.merge(temp, on='CUST_NO', how='left')\n",
    "# ratio\n",
    "for i in jobinfo.ORG_NO.unique(): \n",
    "    df['ratio_ORG_NO_{}'.format(i)] = df['count_ORG_NO_{}'.format(i)] / df.counts_of_jobinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "len_of_ORG_NO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "jobinfo['len_of_ORG_NO'] = jobinfo.ORG_NO.apply(lambda x:len(str(x)))\n",
    "df['nunique_len_of_ORG_NO'] = df.CUST_NO.map(jobinfo.groupby('CUST_NO').len_of_ORG_NO.nunique())\n",
    "# count\n",
    "temp = jobinfo[['CUST_NO']]\n",
    "temp = pd.concat([temp, pd.get_dummies(jobinfo.len_of_ORG_NO, prefix='count_len_of_ORG_NO')], axis=1)\n",
    "temp = temp.groupby('CUST_NO').sum()\n",
    "temp.reset_index(inplace=True)\n",
    "df = df.merge(temp, on='CUST_NO', how='left')\n",
    "# ratio\n",
    "for i in jobinfo.len_of_ORG_NO.unique(): \n",
    "    df['ratio_len_of_ORG_NO_{}'.format(i)] = df['count_len_of_ORG_NO_{}'.format(i)] / df.counts_of_jobinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ELEC_TYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "jobinfo['ELEC_TYPE'].fillna(0, inplace=True)\n",
    "df['nunique_ELEC_TYPE'] = df.CUST_NO.map(jobinfo.groupby('CUST_NO').ELEC_TYPE.nunique())\n",
    "df['ratio_ELEC_TYPE'] =  df['counts_of_jobinfo'] / df['nunique_ELEC_TYPE']\n",
    "# count\n",
    "temp = jobinfo[['CUST_NO']]\n",
    "temp = pd.concat([temp, pd.get_dummies(jobinfo.ELEC_TYPE, prefix='count_ELEC_TYPE')], axis=1)\n",
    "temp = temp.groupby('CUST_NO').sum()\n",
    "temp.reset_index(inplace=True)\n",
    "df = df.merge(temp, on='CUST_NO', how='left')\n",
    "# ratio\n",
    "for i in jobinfo.ELEC_TYPE.unique(): \n",
    "    df['ratio_ELEC_TYPE_{}'.format(i)] = df['count_ELEC_TYPE_{}'.format(i)] / df.counts_of_jobinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Head_of_ELEC_TYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "jobinfo['head_of_ELEC_TYPE'] = jobinfo.ELEC_TYPE.apply(lambda x: int(str(x)[0]))\n",
    "df['nunique_head_of_ELEC_TYPE'] = df.CUST_NO.map(jobinfo.groupby('CUST_NO').head_of_ELEC_TYPE.nunique())\n",
    "df['ratio_head_of_ELEC_TYPE'] =  df['counts_of_jobinfo'] / df['nunique_head_of_ELEC_TYPE']\n",
    "# count\n",
    "temp = jobinfo[['CUST_NO']]\n",
    "temp = pd.concat([temp, pd.get_dummies(jobinfo.head_of_ELEC_TYPE, prefix='count_head_of_ELEC_TYPE')], axis=1)\n",
    "temp = temp.groupby('CUST_NO').sum()\n",
    "temp.reset_index(inplace=True)\n",
    "df = df.merge(temp, on='CUST_NO', how='left')\n",
    "# ratio\n",
    "for i in jobinfo.head_of_ELEC_TYPE.unique(): \n",
    "    df['ratio_head_of_ELEC_TYPE_{}'.format(i)] = df['count_head_of_ELEC_TYPE_{}'.format(i)] / df.counts_of_jobinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "时间相关特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 1. month12维\n",
    "jobinfo['date'] = jobinfo.HANDLE_TIME.apply(lambda x:pd.to_datetime(str(x).split()[0]))\n",
    "jobinfo['month'] = jobinfo.date.apply(lambda x:x.month)\n",
    "# count\n",
    "temp = jobinfo[['CUST_NO']]\n",
    "temp = pd.concat([temp, pd.get_dummies(jobinfo.month, prefix='count_month')], axis=1)\n",
    "temp = temp.groupby('CUST_NO').sum()\n",
    "temp.reset_index(inplace=True)\n",
    "df = df.merge(temp, on='CUST_NO', how='left')\n",
    "# ratio\n",
    "for i in jobinfo.month.unique(): \n",
    "    df['ratio_month_{}'.format(i)] = df['count_month_{}'.format(i)] / df.counts_of_jobinfo\n",
    "# 2. 几个不同的日期\n",
    "df['nunique_date'] = df.CUST_NO.map(jobinfo.groupby('CUST_NO').date.nunique())\n",
    "# 3.第一个电话和最后一个电话间隔几天\n",
    "df['dates'] = df.CUST_NO.map(jobinfo.groupby('CUST_NO').date.apply(lambda x:[i for i in sorted(x)]))\n",
    "df['how_many_days_interval'] = df.dates.apply(lambda x:x[-1] - x[0])\n",
    "df['how_many_days_interval'] = df.how_many_days_interval.apply(lambda x:x.days)\n",
    "df.drop(['dates'], axis=1, inplace=True)\n",
    "# 4.平均几天打一个电话\n",
    "df['how_many_days_one_call'] = df['how_many_days_interval'] / df['counts_of_jobinfo']\n",
    "# 5.平均一天打几个电话\n",
    "df['how_many_calls_in_oneday'] = df['counts_of_jobinfo'] / df['nunique_date']\n",
    "# 6.平均多少天会打电话\n",
    "df['mean_how_many_days_call'] = df['how_many_days_interval'] / df['nunique_date']\n",
    "# 7. 间隔\n",
    "jobinfo['time'] = jobinfo.HANDLE_TIME.apply(lambda x:pd.to_datetime(x))\n",
    "df['times'] = df.CUST_NO.map(jobinfo.groupby('CUST_NO').time.apply(lambda x:[i for i in sorted(x)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_gaps(x):\n",
    "    gaps = []\n",
    "    for i in range(len(x)-1):\n",
    "        gap = pd.to_datetime(x[i+1]) - pd.to_datetime(x[i])\n",
    "        gaps.append(gap.days)\n",
    "    return gaps\n",
    "df['gaps'] = df.times.apply(lambda x:get_gaps(x))\n",
    "df['min_gap'] = df.gaps.apply(lambda x:min(x))\n",
    "df['max_gap'] = df.gaps.apply(lambda x:max(x))\n",
    "df['mean_gap'] = df.gaps.apply(lambda x:np.mean(x))\n",
    "df['std_gap'] = df.gaps.apply(lambda x:np.std(x))\n",
    "df['median_gap'] = df.gaps.apply(lambda x:np.median(x))\n",
    "df.drop(['times', 'gaps'], axis=1, inplace=True)\n",
    "# 8. 一个月中的哪一天\n",
    "jobinfo['day'] = jobinfo.date.apply(lambda x:x.day)\n",
    "temp = jobinfo[['CUST_NO']]\n",
    "temp = pd.concat([temp, pd.get_dummies(jobinfo.day, prefix='day')], axis=1)\n",
    "temp = temp.groupby('CUST_NO').sum()\n",
    "temp.reset_index(inplace=True)\n",
    "df = df.merge(temp, on='CUST_NO', how='left')\n",
    "# 9. 小时\n",
    "jobinfo['hour'] = jobinfo.time.apply(lambda x:x.hour)\n",
    "temp = jobinfo[['CUST_NO']]\n",
    "temp = pd.concat([temp, pd.get_dummies(jobinfo.hour, prefix='hour')], axis=1)\n",
    "temp = temp.groupby('CUST_NO').sum()\n",
    "temp.reset_index(inplace=True)\n",
    "df = df.merge(temp, on='CUST_NO', how='left')\n",
    "# 10. 最多一个月打了几个电话\n",
    "temp = jobinfo[['CUST_NO']]\n",
    "temp = pd.concat([temp, pd.get_dummies(jobinfo.month, prefix='month')], axis=1)\n",
    "temp = temp.groupby('CUST_NO').sum()\n",
    "temp.reset_index(inplace=True)\n",
    "temp = pd.concat([temp, temp.drop(['CUST_NO'], axis=1).max(axis=1).to_frame(name='most_times_jobinfo_in_one_month')], axis=1)\n",
    "df = df.merge(temp[['CUST_NO', 'most_times_jobinfo_in_one_month']], on='CUST_NO', how='left')\n",
    "# 11.打电话日期的标准差\n",
    "jobinfo['day_of_year'] = jobinfo.date.apply(lambda x:x.dayofyear)\n",
    "df['std_day_of_year'] = df.CUST_NO.map(jobinfo.groupby('CUST_NO').day_of_year.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CITY_ORG_NO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df['nunique_CITY_ORG_NO'] = df.CUST_NO.map(jobinfo.groupby('CUST_NO').CITY_ORG_NO.nunique())\n",
    "df['ratio_CITY_ORG_NO'] =  df['counts_of_jobinfo'] / df['nunique_CITY_ORG_NO']\n",
    "# count\n",
    "temp = jobinfo[['CUST_NO']]\n",
    "temp = pd.concat([temp, pd.get_dummies(jobinfo.CITY_ORG_NO, prefix='count_CITY_ORG_NO')], axis=1)\n",
    "temp = temp.groupby('CUST_NO').sum()\n",
    "temp.reset_index(inplace=True)\n",
    "df = df.merge(temp, on='CUST_NO', how='left')\n",
    "# ratio\n",
    "for i in jobinfo.CITY_ORG_NO.unique(): \n",
    "    df['ratio_CITY_ORG_NO_{}'.format(i)] = df['count_CITY_ORG_NO_{}'.format(i)] / df.counts_of_jobinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ratio\n",
    "train = jobinfo[jobinfo.label != -1]\n",
    "ratio = {}\n",
    "a = 0.001\n",
    "for i in train.topic.unique():\n",
    "    ratio[i] = (len(train.loc[(train.topic == i) & (train.label == 1)]) + a) / (len(train.loc[train.topic == i]) + 2*a)\n",
    "    \n",
    "topics = jobinfo.topic.value_counts().to_frame().reset_index()\n",
    "topics.columns = ['topic', 'counts']\n",
    "\n",
    "topics['multi_topic_ratio'] = topics.topic.map(ratio)\n",
    "topics = topics.loc[(topics.counts > 4) & (~topics.multi_topic_ratio.isnull())]\n",
    "\n",
    "jobinfo = jobinfo.merge(topics[['topic', 'multi_topic_ratio']], on='topic', how='left')\n",
    "jobinfo.multi_topic_ratio.fillna(topics.multi_topic_ratio.median(), inplace=True)\n",
    "\n",
    "df['sum_multi_topic_ratio'] = df.CUST_NO.map(jobinfo.groupby('CUST_NO').multi_topic_ratio.sum())\n",
    "df['mean_multi_topic_ratio'] = df.CUST_NO.map(jobinfo.groupby('CUST_NO').multi_topic_ratio.mean())\n",
    "df['max_multi_topic_ratio'] = df.CUST_NO.map(jobinfo.groupby('CUST_NO').multi_topic_ratio.max())\n",
    "df['min_multi_topic_ratio'] = df.CUST_NO.map(jobinfo.groupby('CUST_NO').multi_topic_ratio.min())\n",
    "df['std_multi_topic_ratio'] = df.CUST_NO.map(jobinfo.groupby('CUST_NO').multi_topic_ratio.std())\n",
    "df['median_multi_topic_ratio'] = df.CUST_NO.map(jobinfo.groupby('CUST_NO').multi_topic_ratio.median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "电费相关特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "统计特征处理完成！\n"
     ]
    }
   ],
   "source": [
    "train_flow = pd.read_csv(data_path + file_flow_train, sep='\\t')\n",
    "test_flow = pd.read_csv(data_path + file_flow_test, sep='\\t')\n",
    "flow = train_flow.append(test_flow).copy()\n",
    "flow.rename(columns={'CONS_NO':'CUST_NO'}, inplace=True)\n",
    "flow.drop_duplicates(inplace=True)\n",
    "flow = flow.loc[flow.CUST_NO.isin(df.CUST_NO)].copy()\n",
    "\n",
    "flow['T_PQ'] = flow.T_PQ.apply(lambda x:-x if x<0 else x)\n",
    "flow['RCVBL_AMT'] = flow.RCVBL_AMT.apply(lambda x:-x if x<0 else x)\n",
    "flow['RCVED_AMT'] = flow.RCVED_AMT.apply(lambda x:-x if x<0 else x)\n",
    "flow['OWE_AMT'] = flow.OWE_AMT.apply(lambda x:-x if x<0 else x)\n",
    "# 是否有表9\n",
    "df['has_biao9'] = 0\n",
    "df.loc[df.CUST_NO.isin(flow.CUST_NO), 'has_biao9'] = 1\n",
    "\n",
    "df['counts_of_09flow'] = df.CUST_NO.map(flow.groupby('CUST_NO').size())\n",
    "\n",
    "# 应收金额\n",
    "df['sum_yingshoujine'] = log(df.CUST_NO.map(flow.groupby('CUST_NO').RCVBL_AMT.sum()) + 1)\n",
    "df['mean_yingshoujine'] = log(df.CUST_NO.map(flow.groupby('CUST_NO').RCVBL_AMT.mean()) + 1)\n",
    "df['max_yingshoujine'] = log(df.CUST_NO.map(flow.groupby('CUST_NO').RCVBL_AMT.max()) + 1)\n",
    "df['min_yingshoujine'] = log(df.CUST_NO.map(flow.groupby('CUST_NO').RCVBL_AMT.min()) + 1)\n",
    "df['std_yingshoujine'] = log(df.CUST_NO.map(flow.groupby('CUST_NO').RCVBL_AMT.std()) + 1)\n",
    "# 实收金额\n",
    "df['sum_shishoujine'] = log(df.CUST_NO.map(flow.groupby('CUST_NO').RCVED_AMT.sum()) + 1)\n",
    "# 少交了多少\n",
    "df['qianfei'] = df['sum_yingshoujine'] - df['sum_shishoujine']\n",
    "\n",
    "# 总电量\n",
    "df['sum_T_PQ'] = log(df.CUST_NO.map(flow.groupby('CUST_NO').T_PQ.sum()) + 1)\n",
    "df['mean_T_PQ'] = log(df.CUST_NO.map(flow.groupby('CUST_NO').T_PQ.mean()) + 1)\n",
    "df['max_T_PQ'] = log(df.CUST_NO.map(flow.groupby('CUST_NO').T_PQ.max()) + 1)\n",
    "df['min_T_PQ'] = log(df.CUST_NO.map(flow.groupby('CUST_NO').T_PQ.min()) + 1)\n",
    "df['std_T_PQ'] = log(df.CUST_NO.map(flow.groupby('CUST_NO').T_PQ.std()) + 1)\n",
    "\n",
    "# 电费金额\n",
    "df['sum_OWE_AMT'] = log(df.CUST_NO.map(flow.groupby('CUST_NO').OWE_AMT.sum()) + 1)\n",
    "df['mean_OWE_AMT'] = log(df.CUST_NO.map(flow.groupby('CUST_NO').OWE_AMT.mean()) + 1)\n",
    "df['max_OWE_AMT'] = log(df.CUST_NO.map(flow.groupby('CUST_NO').OWE_AMT.max()) + 1)\n",
    "df['min_OWE_AMT'] = log(df.CUST_NO.map(flow.groupby('CUST_NO').OWE_AMT.min()) + 1)\n",
    "df['std_OWE_AMT'] = log(df.CUST_NO.map(flow.groupby('CUST_NO').OWE_AMT.std()) + 1)\n",
    "\n",
    "# 电费金额和应收金额差多少\n",
    "df['dianfei_jian_yingshoujine'] = df['sum_OWE_AMT'] - df['sum_yingshoujine']\n",
    "\n",
    "# 应收违约金\n",
    "df['sum_RCVBL_PENALTY'] = log(df.CUST_NO.map(flow.groupby('CUST_NO').RCVBL_PENALTY.sum()) + 1)\n",
    "df['mean_RCVBL_PENALTY'] = log(df.CUST_NO.map(flow.groupby('CUST_NO').RCVBL_PENALTY.mean()) + 1)\n",
    "df['max_RCVBL_PENALTY'] = log(df.CUST_NO.map(flow.groupby('CUST_NO').RCVBL_PENALTY.max()) + 1)\n",
    "df['min_RCVBL_PENALTY'] = log(df.CUST_NO.map(flow.groupby('CUST_NO').RCVBL_PENALTY.min()) + 1)\n",
    "df['std_RCVBL_PENALTY'] = log(df.CUST_NO.map(flow.groupby('CUST_NO').RCVBL_PENALTY.std()) + 1)\n",
    "\n",
    "# 实收违约金\n",
    "df['sum_RCVED_PENALTY'] = log(df.CUST_NO.map(flow.groupby('CUST_NO').RCVED_PENALTY.sum()) + 1)\n",
    "df['mean_RCVED_PENALTY'] = log(df.CUST_NO.map(flow.groupby('CUST_NO').RCVED_PENALTY.mean()) + 1)\n",
    "df['max_RCVED_PENALTY'] = log(df.CUST_NO.map(flow.groupby('CUST_NO').RCVED_PENALTY.max()) + 1)\n",
    "df['min_RCVED_PENALTY'] = log(df.CUST_NO.map(flow.groupby('CUST_NO').RCVED_PENALTY.min()) + 1)\n",
    "df['std_RCVED_PENALTY'] = log(df.CUST_NO.map(flow.groupby('CUST_NO').RCVED_PENALTY.std()) + 1)\n",
    "\n",
    "df['chaduoshao_weiyuejin'] = df['sum_RCVBL_PENALTY'] - df['sum_RCVED_PENALTY']\n",
    "\n",
    "# 每个用户有几个月的记录\n",
    "df['nunique_RCVBL_YM'] = df.CUST_NO.map(flow.groupby('CUST_NO').RCVBL_YM.nunique())\n",
    "\n",
    "# 平均每个月几条\n",
    "df['mean_RCVBL_YM'] = df['counts_of_09flow'] / df['nunique_RCVBL_YM']\n",
    "del train_flow, test_flow, flow\n",
    "\n",
    "print('统计特征处理完成！')\n",
    "pickle.dump(df, open('../myfeatures/statistical_features_2.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文本特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.527 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分词ing...\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "mywords = ['户号', '分时', '抄表', '抄表示数', '工单', '单号', '工单号', '空气开关', '脉冲灯', '计量表', '来电', '报修']\n",
    "for word in mywords:\n",
    "    jieba.add_word(word)\n",
    "\n",
    "stops = set()\n",
    "with open('../stopwords.txt', encoding='utf-8')as f:\n",
    "    for word in f:\n",
    "        word = word.strip()\n",
    "        stops.add(word)\n",
    "\n",
    "def fenci(line):\n",
    "    res = []\n",
    "    words = jieba.cut(line)\n",
    "    for word in words:\n",
    "        if word not in stops:\n",
    "            res.append(word)\n",
    "    return ' '.join(res)\n",
    "print('分词ing...')\n",
    "jobinfo['contents'] = jobinfo.ACCEPT_CONTENT.apply(lambda x:fenci(x))\n",
    "def hash_number(x):\n",
    "    shouji_pattern = re.compile('\\s1\\d{10}\\s|\\s1\\d{10}\\Z')\n",
    "    if shouji_pattern.findall(x):\n",
    "        x = re.sub(shouji_pattern, ' 手机number ', x)\n",
    "    \n",
    "    huhao_pattern = re.compile('\\s\\d{10}\\s|\\s\\d{10}\\Z')\n",
    "    if huhao_pattern.findall(x):\n",
    "        x = re.sub(huhao_pattern, ' 户号number ', x)\n",
    "            \n",
    "    tuiding_pattern = re.compile('\\s\\d{11}\\s|\\s\\d{11}\\Z')\n",
    "    if tuiding_pattern.findall(x):\n",
    "        x = re.sub(tuiding_pattern, ' 退订number ', x)\n",
    "            \n",
    "    gongdan_pattern = re.compile('\\s201\\d{13}\\s|\\s201\\d{13}\\Z')\n",
    "    if gongdan_pattern.findall(x):\n",
    "        x = re.sub(gongdan_pattern, ' 工单number ', x)\n",
    "            \n",
    "    tingdian_pattern = re.compile('\\s\\d{12}\\s|\\s\\d{12}\\Z')\n",
    "    if tingdian_pattern.findall(x):\n",
    "        x = re.sub(tingdian_pattern, ' 停电number ', x)\n",
    "        \n",
    "    return x.strip()\n",
    "jobinfo['contents'] = jobinfo['contents'].apply(lambda x:hash_number(x))\n",
    "\n",
    "text = df[['CUST_NO', 'counts_of_jobinfo']].copy()\n",
    "text['contents'] = text.CUST_NO.map(jobinfo.groupby('CUST_NO').contents.apply(lambda x:' '.join(x)))\n",
    "\n",
    "jobinfo['len_of_contents'] = jobinfo.contents.apply(lambda x:len(x.split()))\n",
    "jobinfo['counts_of_words'] = jobinfo.contents.apply(lambda x:len(set(x.split())))\n",
    "\n",
    "text['max_len_of_content'] = text.CUST_NO.map(jobinfo.groupby('CUST_NO').len_of_contents.max())\n",
    "text['min_len_of_content'] = text.CUST_NO.map(jobinfo.groupby('CUST_NO').len_of_contents.min())\n",
    "text['sum_len_of_content'] = text.CUST_NO.map(jobinfo.groupby('CUST_NO').len_of_contents.sum())\n",
    "text['std_len_of_content'] = text.CUST_NO.map(jobinfo.groupby('CUST_NO').len_of_contents.std())\n",
    "text['median_len_of_content'] = text.CUST_NO.map(jobinfo.groupby('CUST_NO').len_of_contents.median())\n",
    "text['mean_len_of_content'] = text.sum_len_of_content / text.counts_of_jobinfo\n",
    "\n",
    "text['sum_counts_of_words'] = text.contents.apply(lambda x:len(set(x.split())))\n",
    "text['mean_counts_of_words'] = text.sum_counts_of_words / text.counts_of_jobinfo\n",
    "\n",
    "text.drop(['counts_of_jobinfo'], axis=1, inplace=True)\n",
    "\n",
    "pickle.dump(text, open('../myfeatures/text_features_2.pkl', 'wb'))\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本特征选择"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select features...\n",
      "tfidf...\n",
      "文本特征：39457维\n",
      "其他特征：648维\n",
      "-----------------------------------------------------------------\n",
      "特征数量 40105\n",
      "采用xgboost筛选文本特征...\n",
      "training...\n",
      "[0]\ttrain-error:0.092563\n",
      "Will train until train-error hasn't improved in 200 rounds.\n",
      "[200]\ttrain-error:0.074985\n",
      "[400]\ttrain-error:0.062242\n",
      "[600]\ttrain-error:0.050371\n",
      "[800]\ttrain-error:0.039981\n",
      "[1000]\ttrain-error:0.030783\n",
      "[1200]\ttrain-error:0.023691\n",
      "[1400]\ttrain-error:0.018296\n",
      "[1600]\ttrain-error:0.013873\n",
      "[1800]\ttrain-error:0.010678\n",
      "[2000]\ttrain-error:0.008202\n",
      "[2200]\ttrain-error:0.006279\n",
      "[2400]\ttrain-error:0.004668\n",
      "[2600]\ttrain-error:0.003487\n",
      "[2800]\ttrain-error:0.002642\n",
      "[3000]\ttrain-error:0.001896\n",
      "[3200]\ttrain-error:0.001418\n",
      "[3400]\ttrain-error:0.001098\n",
      "[3600]\ttrain-error:0.000814\n",
      "[3800]\ttrain-error:0.000581\n",
      "[4000]\ttrain-error:0.00043\n",
      "[4200]\ttrain-error:0.000312\n",
      "[4400]\ttrain-error:0.000229\n",
      "[4600]\ttrain-error:0.000182\n",
      "[4800]\ttrain-error:0.000138\n",
      "[5000]\ttrain-error:0.000103\n",
      "[5200]\ttrain-error:5.9e-05\n",
      "[5400]\ttrain-error:2.8e-05\n",
      "[5600]\ttrain-error:1.6e-05\n",
      "[5800]\ttrain-error:8e-06\n",
      "Stopping. Best iteration:\n",
      "[5777]\ttrain-error:8e-06\n",
      "\n",
      "训练完毕。\n",
      "留下文本特征数量： 988\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from scipy.sparse import csc_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import xgboost as xgb\n",
    "\n",
    "print('select features...')\n",
    "\n",
    "df = pickle.load(open('../myfeatures/statistical_features_2.pkl', 'rb'))\n",
    "text = pickle.load(open('../myfeatures/text_features_2.pkl', 'rb'))\n",
    "df = df.merge(text, on='CUST_NO', how='left')\n",
    "\n",
    "train = df.loc[df.label != -1]\n",
    "test = df.loc[df.label == -1]\n",
    "\n",
    "x_data = train.copy()\n",
    "x_data = x_data.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "x_val = test.copy()\n",
    "#################\n",
    "#################\n",
    "###   input   ###\n",
    "#################\n",
    "#################\n",
    "delete_columns = ['CUST_NO', 'label', 'contents']\n",
    "X_train_1 = csc_matrix(x_data.drop(delete_columns, axis=1).as_matrix())\n",
    "X_val_1 = csc_matrix(x_val.drop(delete_columns, axis=1).as_matrix())\n",
    "y_train = x_data.label.values\n",
    "y_val = x_val.label.values\n",
    "featurenames = list(x_data.drop(delete_columns, axis=1).columns)\n",
    "print('tfidf...')\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), min_df=3, use_idf=False, smooth_idf=False, sublinear_tf=True)\n",
    "tfidf.fit(x_data.contents)\n",
    "word_names = tfidf.get_feature_names()\n",
    "print('文本特征：{}维'.format(len(word_names)))\n",
    "X_train_2 = tfidf.transform(x_data.contents)\n",
    "X_val_2 = tfidf.transform(x_val.contents)\n",
    "\n",
    "statistic_feature = featurenames.copy()\n",
    "print('其他特征：{}维'.format(len(statistic_feature)))\n",
    "print('-----------------------------------------------------------------')\n",
    "featurenames.extend(word_names)\n",
    "from scipy.sparse import hstack\n",
    "X_train = hstack(((X_train_1), (X_train_2))).tocsc()\n",
    "X_val = hstack(((X_val_1), (X_val_2))).tocsc()\n",
    "\n",
    "print('特征数量',X_train.shape[1])\n",
    "\n",
    "##############\n",
    "#  xgboost\n",
    "##############\n",
    "print('采用xgboost筛选文本特征...')\n",
    "print('training...')\n",
    "dtrain = xgb.DMatrix(X_train, y_train, feature_names=featurenames)\n",
    "dval = xgb.DMatrix(X_val, feature_names=featurenames)\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"booster\": \"gbtree\",\n",
    "    \"eval_metric\": \"error\",\n",
    "    'max_depth':10,\n",
    "    'subsample':0.8,\n",
    "    'min_child_weight':5,\n",
    "    'colsample_bytree':1,\n",
    "    'gamma':0.2,\n",
    "    \"eta\": 0.1,\n",
    "    \"lambda\":300,\n",
    "    'alpha':0,\n",
    "    \"silent\": 1,\n",
    "    'seed':1,\n",
    "}\n",
    "watchlist = [(dtrain, 'train')]\n",
    "\n",
    "model = xgb.train(params, dtrain, 6666, evals=watchlist,\n",
    "                early_stopping_rounds=200, verbose_eval=200)\n",
    "print('训练完毕。')\n",
    "temp = pd.DataFrame.from_dict(model.get_fscore(), orient='index').reset_index()\n",
    "temp.columns = ['feature', 'score']\n",
    "temp.sort_values(['score'], axis=0, ascending=False, inplace=True)\n",
    "temp.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print('留下文本特征数量：', len(temp.loc[~temp.feature.isin(statistic_feature)]))\n",
    "\n",
    "selected_words = list(temp.loc[~temp.feature.isin(statistic_feature)].feature.values)\n",
    "pickle.dump(selected_words, open('../myfeatures/multi_select_words.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建高敏模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training model...\n",
      "训练集： 253223\n",
      "正样本: 64139\n",
      "负样本: 189084\n",
      "-----------------------\n",
      "测试集： 43390\n",
      "-----------------------\n",
      "tfidf...\n",
      "文本特征：988维\n",
      "其他特征：648维\n",
      "特征数量 1636\n",
      "-----------------------------------------------------------------\n",
      "start 3 xgboost!\n",
      "group: 1\n",
      "training...\n",
      "[0]\ttrain-error:0.088432\n",
      "Will train until train-error hasn't improved in 50 rounds.\n",
      "[100]\ttrain-error:0.074184\n",
      "[200]\ttrain-error:0.062925\n",
      "[300]\ttrain-error:0.052531\n",
      "[400]\ttrain-error:0.042603\n",
      "[500]\ttrain-error:0.034393\n",
      "[600]\ttrain-error:0.027596\n",
      "[700]\ttrain-error:0.022186\n",
      "[800]\ttrain-error:0.017633\n",
      "[900]\ttrain-error:0.014197\n",
      "[1000]\ttrain-error:0.011318\n",
      "[1100]\ttrain-error:0.008803\n",
      "[1200]\ttrain-error:0.007136\n",
      "[1300]\ttrain-error:0.005556\n",
      "[1400]\ttrain-error:0.004545\n",
      "[1500]\ttrain-error:0.003613\n",
      "[1600]\ttrain-error:0.002907\n",
      "[1700]\ttrain-error:0.002417\n",
      "[1800]\ttrain-error:0.0019\n",
      "[1900]\ttrain-error:0.001501\n",
      "[1999]\ttrain-error:0.001125\n",
      "predicting...\n",
      "----------------------------------\n",
      "group: 2\n",
      "training...\n",
      "[0]\ttrain-error:0.087366\n",
      "Will train until train-error hasn't improved in 50 rounds.\n",
      "[100]\ttrain-error:0.074136\n",
      "[200]\ttrain-error:0.062917\n",
      "[300]\ttrain-error:0.052365\n",
      "[400]\ttrain-error:0.042923\n",
      "[500]\ttrain-error:0.034649\n",
      "[600]\ttrain-error:0.027806\n",
      "[700]\ttrain-error:0.021957\n",
      "[800]\ttrain-error:0.017439\n",
      "[900]\ttrain-error:0.013956\n",
      "[1000]\ttrain-error:0.010907\n",
      "[1100]\ttrain-error:0.008799\n",
      "[1200]\ttrain-error:0.007029\n",
      "[1300]\ttrain-error:0.005533\n",
      "[1400]\ttrain-error:0.004486\n",
      "[1500]\ttrain-error:0.003598\n",
      "[1600]\ttrain-error:0.002847\n",
      "[1700]\ttrain-error:0.002334\n",
      "[1800]\ttrain-error:0.001793\n",
      "[1900]\ttrain-error:0.001461\n",
      "[1999]\ttrain-error:0.001169\n",
      "predicting...\n",
      "----------------------------------\n",
      "group: 3\n",
      "training...\n",
      "[0]\ttrain-error:0.087626\n",
      "Will train until train-error hasn't improved in 50 rounds.\n",
      "[100]\ttrain-error:0.074417\n",
      "[200]\ttrain-error:0.063304\n",
      "[300]\ttrain-error:0.052475\n",
      "[400]\ttrain-error:0.042931\n",
      "[500]\ttrain-error:0.03463\n",
      "[600]\ttrain-error:0.02743\n",
      "[700]\ttrain-error:0.021838\n",
      "[800]\ttrain-error:0.017795\n",
      "[900]\ttrain-error:0.014165\n",
      "[1000]\ttrain-error:0.011354\n",
      "[1100]\ttrain-error:0.00896\n",
      "[1200]\ttrain-error:0.007219\n",
      "[1300]\ttrain-error:0.005793\n",
      "[1400]\ttrain-error:0.004628\n",
      "[1500]\ttrain-error:0.003673\n",
      "[1600]\ttrain-error:0.003033\n",
      "[1700]\ttrain-error:0.002425\n",
      "[1800]\ttrain-error:0.001923\n",
      "[1900]\ttrain-error:0.001532\n",
      "[1999]\ttrain-error:0.001204\n",
      "predicting...\n",
      "----------------------------------\n",
      "gl!\n",
      "merge...\n",
      "finish!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csc_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import xgboost as xgb\n",
    "\n",
    "def threshold(y, t):\n",
    "    z = np.copy(y)\n",
    "    z[z>=t] = 1\n",
    "    z[z<t] = 0\n",
    "    return z\n",
    "print('training model...')\n",
    "df = pickle.load(open('../myfeatures/statistical_features_2.pkl', 'rb'))\n",
    "text = pickle.load(open('../myfeatures/text_features_2.pkl', 'rb'))\n",
    "df = df.merge(text, on='CUST_NO', how='left')\n",
    "\n",
    "train = df.loc[df.label != -1]\n",
    "test = df.loc[df.label == -1]\n",
    "print('训练集：',train.shape[0])\n",
    "print('正样本:',train.loc[train.label == 1].shape[0])\n",
    "print('负样本:',train.loc[train.label == 0].shape[0])\n",
    "print('-----------------------')\n",
    "print('测试集：',test.shape[0])\n",
    "print('-----------------------')\n",
    "x_data = train.copy()\n",
    "x_data = x_data.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "x_val = test.copy()\n",
    "#################\n",
    "#################\n",
    "###   input   ###\n",
    "#################\n",
    "#################\n",
    "delete_columns = ['CUST_NO', 'label', 'contents']\n",
    "X_train_1 = csc_matrix(x_data.drop(delete_columns, axis=1).as_matrix())\n",
    "X_val_1 = csc_matrix(x_val.drop(delete_columns, axis=1).as_matrix())\n",
    "y_train = x_data.label.values\n",
    "y_val = x_val.label.values\n",
    "featurenames = list(x_data.drop(delete_columns, axis=1).columns)\n",
    "\n",
    "print('tfidf...')\n",
    "select_words = pickle.load(open('../myfeatures/multi_select_words.pkl', 'rb'))\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), min_df=3, use_idf=False, smooth_idf=False, sublinear_tf=True, vocabulary=select_words)\n",
    "tfidf.fit(x_data.contents)\n",
    "word_names = tfidf.get_feature_names()\n",
    "print('文本特征：{}维'.format(len(word_names)))\n",
    "X_train_2 = tfidf.transform(x_data.contents)\n",
    "X_val_2 = tfidf.transform(x_val.contents)\n",
    "\n",
    "statistic_feature = featurenames.copy()\n",
    "print('其他特征：{}维'.format(len(statistic_feature)))\n",
    "featurenames.extend(word_names)\n",
    "from scipy.sparse import hstack\n",
    "X_train = hstack(((X_train_1), (X_train_2))).tocsc()\n",
    "X_val = hstack(((X_val_1), (X_val_2))).tocsc()\n",
    "print('特征数量',X_train.shape[1])\n",
    "print('-----------------------------------------------------------------')\n",
    "print('start 3 xgboost!')\n",
    "bagging = []\n",
    "for i in range(1,4):\n",
    "    print('group:',i)\n",
    "    ##############\n",
    "    #  xgboost\n",
    "    ##############\n",
    "    print('training...')\n",
    "    dtrain = xgb.DMatrix(X_train, y_train, feature_names=featurenames)\n",
    "    dval = xgb.DMatrix(X_val, feature_names=featurenames)\n",
    "    \n",
    "    params = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"booster\": \"gbtree\",\n",
    "        \"eval_metric\": \"error\",\n",
    "        'max_depth':14,\n",
    "        'subsample':0.8,\n",
    "        'min_child_weight':3,\n",
    "        'colsample_bytree':1,\n",
    "        'gamma':0.2,\n",
    "        \"eta\": 0.1,\n",
    "        \"lambda\":300,\n",
    "        'alpha':0,\n",
    "        \"silent\": 1,\n",
    "        \"seed\":i,\n",
    "    }\n",
    "    watchlist = [(dtrain, 'train')]\n",
    "\n",
    "    model = xgb.train(params, dtrain, 2000, evals=watchlist,\n",
    "                    early_stopping_rounds=50, verbose_eval=100)\n",
    "\n",
    "    print('predicting...')\n",
    "    y_prob = model.predict(dval, ntree_limit=model.best_ntree_limit)\n",
    "    bagging.append(y_prob)\n",
    "    print('----------------------------------')\n",
    "print('gl!')\n",
    "\n",
    "t = 0.35\n",
    "pres = []\n",
    "for i in bagging:\n",
    "    pres.append(threshold(i, t))\n",
    "    \n",
    "# vote\n",
    "pres = np.array(pres).T.astype('int64')\n",
    "result = []\n",
    "for line in pres:\n",
    "    result.append(np.bincount(line).argmax())\n",
    "    \n",
    "myout = test[['CUST_NO']].copy()\n",
    "myout['pre'] = result\n",
    "myout.loc[(myout.pre == 1), 'CUST_NO'].to_csv('../result/B.csv', index=False)\n",
    "\n",
    "print('merge...')\n",
    "single = pd.read_csv('../result/A.csv', header=None, names=['CUST_NO'])\n",
    "multi = pd.read_csv('../result/B.csv', header=None, names=['CUST_NO'])\n",
    "result = single.append(multi)\n",
    "result.loc[:, 'CUST_NO'].to_csv('../result/result.csv', index=False, header=False)\n",
    "print('finish!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
